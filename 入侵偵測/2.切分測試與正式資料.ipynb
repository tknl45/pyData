{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normalized data from HDF5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting X to test and train datasets...\n",
      "Select max 5000 samples for training...\n",
      "analysis : training samples = 2409.0 | total samples = 2677\n",
      "backdoor : training samples = 2096.0 | total samples = 2329\n",
      "dos : training samples = 5000 | total samples = 16353\n",
      "exploits : training samples = 5000 | total samples = 44525\n",
      "fuzzers : training samples = 5000 | total samples = 24246\n",
      "generic : training samples = 5000 | total samples = 215481\n",
      "normal : training samples = 31000 | total samples = 2218456\n",
      "reconnaissance : training samples = 5000 | total samples = 13987\n",
      "shellcode : training samples = 1359.0 | total samples = 1511\n",
      "worms : training samples = 156.0 | total samples = 174\n",
      "Number of categories is 9 | Total samples in categories:\n",
      "| 0: 2677\n",
      "|1: 2329\n",
      "|2: 16353\n",
      "|3: 44525\n",
      "|4: 24246\n",
      "|5: 215481\n",
      "|6: 13987\n",
      "|7: 1511\n",
      "|8: 174\n",
      "normal samples for rf: 31000\n",
      "normal samples for nn: 5000\n",
      "Finding feature importances with ExtraTreesClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving X and Y to HDF5\n"
     ]
    }
   ],
   "source": [
    "print('Loading normalized data from HDF5...')\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas \n",
    "\n",
    "h5f = h5py.File('data.h5', 'r')\n",
    "normalized_X = h5f['normalized_X'].value\n",
    "labeled_Y = h5f['labeled_Y'].value\n",
    "Y = h5f['Y'].value\n",
    "Y_A = h5f['Y_A'].value\n",
    "h5f.close()\n",
    "\n",
    "\n",
    "print('Splitting X to test and train datasets...')\n",
    "# X_test, X_train, Y_test, Y_train = []\n",
    "rf_normal_inds = []\n",
    "nn_normal_inds = []\n",
    "inds = []\n",
    "\n",
    "attack_cats = np.unique(Y)\n",
    "cat_sizes = []\n",
    "np.random.seed(1337)\n",
    "max_training_samples = 5000\n",
    "print('Select max', max_training_samples, 'samples for training...')\n",
    "for cat in attack_cats:\n",
    "    indices = np.ix_(Y == cat)[0]\n",
    "    # total_num_of_samples = indices.shape[0]\n",
    "    np.random.shuffle(indices)\n",
    "    if cat == 'normal':\n",
    "        len_of_subset = min(np.floor(len(indices)*0.9), 31000)\n",
    "        rf_normal_inds = indices[:len_of_subset]\n",
    "        nn_normal_inds = indices[:len_of_subset][:max_training_samples]\n",
    "        cat_size = len(indices)\n",
    "        print(cat, ': training samples =', len_of_subset, '| total samples =', cat_size)\n",
    "    else:\n",
    "        len_of_subset = min(np.floor(len(indices)*0.9), max_training_samples)\n",
    "\n",
    "        cat_size = len(indices)\n",
    "        print(cat, ': training samples =', len_of_subset, '| total samples =', cat_size)\n",
    "        cat_sizes.append(cat_size)\n",
    "        #inds.extend(oversampled_indices)\n",
    "        inds.extend(indices[:int(len_of_subset)])\n",
    "\n",
    "\n",
    "print('Number of categories is', len(cat_sizes), '| Total samples in categories:\\n|', '\\n|'.join([str(i)+': '+str(c) for i, c in enumerate(cat_sizes)]))\n",
    "print('normal samples for rf:', len(rf_normal_inds))\n",
    "print('normal samples for nn:', len(nn_normal_inds))\n",
    "\n",
    "# Attack or not learning data\n",
    "rf_inds = []\n",
    "rf_inds.extend(inds)\n",
    "rf_inds.extend(rf_normal_inds)\n",
    "X_rf_train = normalized_X[rf_inds, :]\n",
    "X_rf_test = np.delete(normalized_X, rf_inds, axis=0)\n",
    "\n",
    "Y_rf_train = Y_A[rf_inds]\n",
    "Y_rf_test = np.delete(Y_A, rf_inds, axis=0)\n",
    "\n",
    "# Category learning data\n",
    "nn_inds = []\n",
    "nn_inds.extend(inds)\n",
    "nn_inds.extend(nn_normal_inds)\n",
    "X_nn_train = normalized_X[nn_inds, :]\n",
    "# Remove rf indices because nn indices is a subset and we dont want to test and train with same data\n",
    "X_nn_test = np.delete(normalized_X, rf_inds, axis=0)\n",
    "\n",
    "del normalized_X\n",
    "\n",
    "Y_nn_train = labeled_Y[nn_inds]\n",
    "Y_nn_train_string = Y[nn_inds]\n",
    "\n",
    "Y_nn_test = np.delete(labeled_Y, rf_inds, axis=0)\n",
    "Y_nn_test_string = np.delete(Y, rf_inds, axis=0)\n",
    "\n",
    "del labeled_Y\n",
    "del Y\n",
    "\n",
    "Y_nn_A_train = Y_A[nn_inds]\n",
    "Y_nn_A_test = np.delete(Y_A, rf_inds, axis=0)\n",
    "\n",
    "del Y_A\n",
    "\n",
    "print('Finding feature importances with ExtraTreesClassifier')\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def find_importances(X_train, Y_train):\n",
    "    model = ExtraTreesClassifier()\n",
    "    model = model.fit(X_train, Y_train)\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]  # Top ranking features' indices\n",
    "    return importances, indices, std\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the feature importances of the forest\n",
    "def plot_feature_importances(X_train, importances, indices, std, title):\n",
    "   #tagy\n",
    "#     for f in range(X_train.shape[1]):\n",
    "#         print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "    plt.figure(num=None, figsize=(18, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.title(title)\n",
    "    width=5\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "          width=5, color=\"r\", yerr=std[indices], align=\"center\") #tagy 1.5 > .8\n",
    "    plt.xticks(range(X_train.shape[1]), indices)\n",
    "    #plt.axis('tight')\n",
    "    plt.xlim([-1, X_train.shape[1]]) # -1 tagy\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "rf_importances, rf_indices, rf_std = find_importances(X_rf_train, Y_rf_train)\n",
    "\n",
    "plot_feature_importances(X_rf_train, rf_importances, rf_indices, rf_std, title='Feature importances (Random forest)')\n",
    "\n",
    "# Neural network is classified with correct 'attack or not' labels\n",
    "X_nn_train = np.concatenate((Y_nn_A_train[:,np.newaxis], X_nn_train), axis=1)\n",
    "nn_importances, nn_indices, nn_std = find_importances(X_nn_train,\n",
    "                                                      Y_nn_train)\n",
    "plot_feature_importances(X_nn_train,\n",
    "                        nn_importances, nn_indices, nn_std, title='Feature importances (Neural network)')\n",
    "\n",
    "NB_RF_FEATURES = 10\n",
    "NB_NN_FEATURES = 25\n",
    "\n",
    "reduced_X_nn_train = X_nn_train[:, nn_indices[0:NB_NN_FEATURES]]\n",
    "reduced_Y_nn_train = Y_nn_train\n",
    "reduced_Y_nn_train_string = Y_nn_train_string\n",
    "reduced_Y_nn_test_string = Y_nn_test_string\n",
    "reduced_Y_nn_A_train = Y_nn_A_train\n",
    "\n",
    "# Test set has 1 less because we get the one from RF\n",
    "\n",
    "reduced_X_nn_test = X_nn_test[:, nn_indices[1:NB_NN_FEATURES]]\n",
    "reduced_Y_nn_test = Y_nn_test\n",
    "\n",
    "\n",
    "reduced_X_rf_train = X_rf_train[:, rf_indices[0:NB_RF_FEATURES]]\n",
    "reduced_Y_rf_train = Y_rf_train\n",
    "\n",
    "reduced_X_rf_test = X_rf_test[:, rf_indices[0:NB_RF_FEATURES]]\n",
    "reduced_Y_rf_test = Y_rf_test\n",
    "\n",
    "\n",
    "\n",
    "print('Saving X and Y to HDF5')\n",
    "import h5py\n",
    "h5f = h5py.File('datasets.h5', 'w')\n",
    "h5f.create_dataset('X_rf_train', data=reduced_X_rf_train)\n",
    "h5f.create_dataset('X_rf_test',  data=reduced_X_rf_test)\n",
    "h5f.create_dataset('Y_rf_train', data=reduced_Y_rf_train)\n",
    "h5f.create_dataset('Y_rf_test',  data=reduced_Y_rf_test)\n",
    "\n",
    "h5f.create_dataset('X_nn_train', data=reduced_X_nn_train)\n",
    "h5f.create_dataset('X_nn_test',  data=reduced_X_nn_test)\n",
    "h5f.create_dataset('Y_nn_train', data=reduced_Y_nn_train)\n",
    "h5f.create_dataset('Y_nn_test',  data=reduced_Y_nn_test)\n",
    "dt = h5py.special_dtype(vlen=str)\n",
    "h5f.create_dataset('Y_nn_train_string', data=reduced_Y_nn_train_string, dtype=dt)\n",
    "h5f.create_dataset('Y_nn_test_string', data=reduced_Y_nn_test_string, dtype=dt)\n",
    "\n",
    "h5f.create_dataset('Y_nn_A_train', data=reduced_Y_nn_A_train)\n",
    "h5f.create_dataset('Y_nn_A_test', data=Y_nn_A_test)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
