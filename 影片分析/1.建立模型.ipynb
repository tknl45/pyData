{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import psutil\n",
    "import sklearn\n",
    "import humanize\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import GPUtil as GPU\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers.core import Permute, Reshape, RepeatVector\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy\n",
    "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply, merge, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "\n",
    "        #Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        #clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.ItemId.unique()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        session_key='SessionId'\n",
    "        item_key='ItemId'\n",
    "        time_key='TimeStamp'\n",
    "        self.n_items = df[item_key].nunique()+1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = idx_input\n",
    "                target = idx_target\n",
    "                yield input, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():   \n",
    "    emb_size = 50\n",
    "    hidden_units = 100\n",
    "    size = emb_size\n",
    "\n",
    "    inputs = Input(batch_shape=(batch_size, 1, n_items))\n",
    "    gru, gru_states = CuDNNGRU(hidden_units, stateful=True, return_state=True)(inputs)# drop1) #\n",
    "    drop2 = Dropout(0.25)(gru)\n",
    "    predictions = Dense(n_items, activation='softmax')(drop2)\n",
    "    model = Model(input=inputs, output=[predictions])\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "    model.summary()\n",
    "\n",
    "    filepath='./DwellTimeModel_checkpoint.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "    callbacks_list = []\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_states(model):\n",
    "    return [K.get_value(s) for s,_ in model.state_updates]\n",
    "\n",
    "\n",
    "def set_states(model, states):\n",
    "    for (d,_), s in zip(model.state_updates, states):\n",
    "        K.set_value(d, s)\n",
    "\n",
    "\n",
    "def get_recall(model, loader, epoch, train_generator_map, recall_k=20):\n",
    "\n",
    "    test_dataset = SessionDataset(test_data, itemmap=train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    suma_baseline = 0\n",
    "\n",
    "    for feat, label, mask in test_generator:\n",
    "\n",
    "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "        target_oh = to_categorical(label, num_classes=loader.n_items)\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        if n%100 == 0:\n",
    "            try:\n",
    "                print(\"{}:{}\".format(n, suma/n))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx] \n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
    "            idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "            if idx2[0] in idx1:\n",
    "                suma += 1\n",
    "\n",
    "    print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))\n",
    "\n",
    "\n",
    "def get_mrr(model, loader,epoch,train_generator_map, mrr_k=20):\n",
    "\n",
    "    test_dataset = SessionDataset(test_data, itemmap = train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    suma_baseline = 0\n",
    "\n",
    "    for feat, label, mask in test_generator:\n",
    "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "        target_oh = to_categorical(label, num_classes=loader.n_items)\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        if n%100 == 0:\n",
    "            try:\n",
    "                print(\"{}:{}\".format(n, suma/n))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx] \n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            idx1 = pred_row.argsort()[-mrr_k:][::-1]\n",
    "            idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "            if idx2[0] in idx1:\n",
    "                suma += 1/int((np.where(idx1 == idx2[0])[0]+1))        \n",
    "\n",
    "    print(\"MRR@{} epoch {}: {}\".format(mrr_k, epoch, suma/n))\n",
    "\n",
    "\n",
    "def train_model(model, save_weights = False, path_to_weights = True):\n",
    "    train_dataset = SessionDataset(train_data)\n",
    "\n",
    "    model_to_train = model\n",
    "\n",
    "    with tqdm(total=train_samples_qty) as pbar:\n",
    "        for epoch in range(1, 10):\n",
    "            if path_to_weights:\n",
    "                loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "            for feat, target, mask in loader:\n",
    "\n",
    "                input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "                input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "                target_oh = to_categorical(target, num_classes=loader.n_items)\n",
    "\n",
    "                tr_loss = model_to_train.train_on_batch(input_oh, target_oh)\n",
    "\n",
    "                real_mask = np.ones((batch_size, 1))\n",
    "                for elt in mask:\n",
    "                    real_mask[elt, :] = 0\n",
    "\n",
    "                hidden_states = get_states(model_to_train)[0]\n",
    "\n",
    "                hidden_states = np.multiply(real_mask, hidden_states)\n",
    "                hidden_states = np.array(hidden_states, dtype=np.float32)\n",
    "                model_to_train.layers[1].reset_states(hidden_states)\n",
    "\n",
    "                pbar.set_description(\"Epoch {0}. Loss: {1:.5f}\".format(epoch, tr_loss))\n",
    "                pbar.update(loader.done_sessions_counter)\n",
    "\n",
    "            # get metrics for epoch\n",
    "            get_recall(model_to_train, loader, epoch,train_dataset.itemmap)\n",
    "            get_mrr(model_to_train, loader,epoch, train_dataset.itemmap)\n",
    "\n",
    "            # save model\n",
    "            if save_weights:\n",
    "                model_to_train.save('./DwellTimeEpoch{}.h5'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training items: 11619\n",
      "Unique dev items: 10105\n",
      "Unique testing items: 10366\n",
      "Training sessions: 19853\n",
      "Dev sessions: 5749\n",
      "Testing sessions: 5271\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (512, 1, 11619)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       [(512, 100), (512, 100)]  3516300   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (512, 100)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (512, 11619)              1173519   \n",
      "=================================================================\n",
      "Total params: 4,689,819\n",
      "Trainable params: 4,689,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "  0%|          | 0/19853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 4.89693:  98%|█████████▊| 19415/19853 [03:03<00:04, 102.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.10875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1. Loss: 4.89693:  98%|█████████▊| 19415/19853 [03:20<00:04, 102.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.1051953125\n",
      "38400:0.10296875\n",
      "51200:0.10345703125\n",
      "64000:0.102375\n",
      "76800:0.103046875\n",
      "89600:0.10227678571428571\n",
      "102400:0.102998046875\n",
      "115200:0.1040625\n",
      "128000:0.10440625\n",
      "140800:0.10494318181818182\n",
      "153600:0.10510416666666667\n",
      "Recall@20 epoch 1: 0.1052538430420712\n",
      "12800:0.02166627194930272\n",
      "25600:0.021169106405827462\n",
      "38400:0.020763122571912897\n",
      "51200:0.020649567897403923\n",
      "64000:0.02033633750157465\n",
      "76800:0.02056439907020205\n",
      "89600:0.020469091788285477\n",
      "102400:0.020719204040481155\n",
      "115200:0.02095348915162105\n",
      "128000:0.02104295194520471\n",
      "140800:0.021173483739982166\n",
      "153600:0.021157853603481235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 7.16843:  98%|█████████▊| 19415/19853 [07:27<00:04, 102.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 1: 0.021190535059394975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 4.98684: : 38823it [10:08, 119.86it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.098125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2. Loss: 4.98684: : 38830it [10:20, 119.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.0921484375\n",
      "38400:0.09044270833333333\n",
      "51200:0.09078125\n",
      "64000:0.08984375\n",
      "76800:0.08985677083333334\n",
      "89600:0.08930803571428571\n",
      "102400:0.08943359375\n",
      "115200:0.08989583333333333\n",
      "128000:0.0899921875\n",
      "140800:0.09\n",
      "153600:0.09006510416666667\n",
      "Recall@20 epoch 2: 0.08999544902912622\n",
      "12800:0.019545199556501042\n",
      "25600:0.019205847463375603\n",
      "38400:0.018361507141115403\n",
      "51200:0.018449809049048114\n",
      "64000:0.018209048366386704\n",
      "76800:0.01825272998695049\n",
      "89600:0.018197605684662072\n",
      "102400:0.01831909759808445\n",
      "115200:0.018418418650165767\n",
      "128000:0.018471468926730163\n",
      "140800:0.0185290895828098\n",
      "153600:0.01857808712122415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Loss: 7.26977: : 38830it [14:35,  8.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 2: 0.01858347535047806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Loss: 4.75798: : 41225it [14:52, 122.53it/s]"
     ]
    }
   ],
   "source": [
    "    PATH_TO_TRAIN = 'processed_augmented_train.csv'\n",
    "    PATH_TO_DEV = 'processed_dev.csv'\n",
    "    PATH_TO_TEST = 'processed_test.csv'\n",
    "    train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "    dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
    "    test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})\n",
    "    \n",
    "    batch_size = 512\n",
    "    session_max_len = 100\n",
    "    embeddingp=False\n",
    "\n",
    "    n_items = len(train_data['ItemId'].unique())+1\n",
    "    print(\"Unique training items:\", n_items)\n",
    "\n",
    "    dev_n_items = len(dev_data['ItemId'].unique())+1\n",
    "    print(\"Unique dev items:\", dev_n_items)\n",
    "\n",
    "    test_n_items = len(test_data['ItemId'].unique())+1\n",
    "    print(\"Unique testing items:\", test_n_items)\n",
    "\n",
    "    train_samples_qty = len(train_data['SessionId'].unique()) \n",
    "    print(\"Training sessions:\", train_samples_qty)\n",
    "\n",
    "    dev_samples_qty = len(dev_data['SessionId'].unique()) \n",
    "    print(\"Dev sessions:\",dev_samples_qty)\n",
    "\n",
    "    test_samples_qty = len(test_data['SessionId'].unique())\n",
    "    print(\"Testing sessions:\", test_samples_qty)\n",
    "    \n",
    "    train_fraction = 1 # (1 / fraction) most recent session quantity to consider\n",
    "    dev_fraction = 1\n",
    "\n",
    "    train_offset_step=train_samples_qty//batch_size\n",
    "    dev_offset_step=dev_samples_qty//batch_size\n",
    "    test_offset_step=test_samples_qty//batch_size\n",
    "    aux = [0]\n",
    "    aux.extend(list(train_data['ItemId'].unique()))\n",
    "    itemids = np.array(aux)\n",
    "    itemidmap = pd.Series(data=np.arange(n_items), index=itemids) \n",
    "    \n",
    "    model = create_model()\n",
    "    \n",
    "    train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
