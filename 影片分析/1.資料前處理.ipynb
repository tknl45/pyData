{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ratings_df = pd.read_csv('ratings.csv')\n",
    "\n",
    "    # ALL TRAIN SET\n",
    "    # Get only ratings between January 2008 to March 2013\n",
    "    all_train_start = \"09/01/1995\"\n",
    "    all_train_end = \"01/03/2013\"\n",
    "    all_train_start_ts = time.mktime(dt.datetime.strptime(all_train_start, \"%d/%m/%Y\").timetuple())\n",
    "    all_train_end_ts = time.mktime(dt.datetime.strptime(all_train_end, \"%d/%m/%Y\").timetuple())\n",
    "    all_train_data = ratings_df.drop(['rating'], axis=1)\n",
    "\n",
    "    # in date range\n",
    "    all_train_data = all_train_data.loc[(all_train_data['timestamp'] >= all_train_start_ts) & (all_train_data['timestamp'] <= all_train_end_ts)]\n",
    "    \n",
    "    # only users 5 < rated_movies < 101\n",
    "    all_train_data = all_train_data.groupby(\"userId\").filter(lambda x: len(x) > 5 and len(x) < 101)\n",
    "\n",
    "    # RECENT TRAIN SET\n",
    "    # Get only ratings between January 2008 to March 2013\n",
    "    train_start = \"01/01/2008\"\n",
    "    train_end = \"01/03/2013\"\n",
    "    train_start_ts = time.mktime(dt.datetime.strptime(train_start, \"%d/%m/%Y\").timetuple())\n",
    "    train_end_ts = time.mktime(dt.datetime.strptime(train_end, \"%d/%m/%Y\").timetuple())\n",
    "    train_data = ratings_df.drop(['rating'], axis=1)\n",
    "\n",
    "    # in date range\n",
    "    train_data = train_data.loc[(train_data['timestamp'] >= train_start_ts) & (train_data['timestamp'] <= train_end_ts)]\n",
    "    # only users 5 < rated_movies < 101\n",
    "    train_data = train_data.groupby(\"userId\").filter(lambda x: len(x) > 5 and len(x) < 101)\n",
    "\n",
    "    # DEV SET\n",
    "    # Get only ratings between April 2014 to April 2015\n",
    "    dev_start = \"01/04/2013\"\n",
    "    dev_end = \"01/04/2014\"\n",
    "    dev_start_ts = time.mktime(dt.datetime.strptime(dev_start, \"%d/%m/%Y\").timetuple())\n",
    "    dev_end_ts = time.mktime(dt.datetime.strptime(dev_end, \"%d/%m/%Y\").timetuple())\n",
    "    dev_data = ratings_df.drop(['rating'], axis=1)\n",
    "\n",
    "    # in date range\n",
    "    dev_data = dev_data.loc[(dev_data['timestamp'] >= dev_start_ts) & (dev_data['timestamp'] <= dev_end_ts)]\n",
    "    # only users 5 < rated_movies < 101\n",
    "    dev_data = dev_data.groupby(\"userId\").filter(lambda x: len(x) > 5 and len(x) < 101)\n",
    "\n",
    "    # TEST SET\n",
    "    # Get only ratings between April 2015 to April 2016\n",
    "    test_start = \"02/04/2014\"\n",
    "    test_end = \"01/04/2015\"\n",
    "    test_start_ts = time.mktime(dt.datetime.strptime(test_start, \"%d/%m/%Y\").timetuple())\n",
    "    test_end_ts = time.mktime(dt.datetime.strptime(test_end, \"%d/%m/%Y\").timetuple())\n",
    "    test_data = ratings_df.drop(['rating'], axis=1)\n",
    "\n",
    "    # in date range\n",
    "    test_data = test_data.loc[(test_data['timestamp'] >= test_start_ts) & (test_data['timestamp'] <= test_end_ts)]\n",
    "    # only users 5 < rated_movies < 101\n",
    "    test_data = test_data.groupby(\"userId\").filter(lambda x: len(x) > 5 and len(x) < 101)\n",
    "\n",
    "    all_train_data.columns = ['SessionId', 'ItemId', 'Time']\n",
    "    train_data.columns = ['SessionId', 'ItemId', 'Time']\n",
    "    dev_data.columns = ['SessionId', 'ItemId', 'Time']\n",
    "    test_data.columns = ['SessionId', 'ItemId', 'Time']\n",
    "\n",
    "    all_train_data.to_csv('processed_all_train.csv', sep='\\t', index=False)\n",
    "    train_data.to_csv('processed_train.csv', sep='\\t', index=False)\n",
    "    dev_data.to_csv('processed_dev.csv', sep='\\t', index=False)\n",
    "    test_data.to_csv('processed_test.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):    \n",
    "    n_items = len(train_data['ItemId'].unique())\n",
    "    aux = list(train_data['ItemId'].unique())\n",
    "    itemids = np.array(aux)\n",
    "    itemidmap = pd.Series(data=np.arange(n_items), index=itemids)  # (id_item => (0, n_items))\n",
    "    \n",
    "    item_key = 'ItemId'\n",
    "    session_key = 'SessionId'\n",
    "    time_key = 'Time'\n",
    "    \n",
    "    data = pd.merge(df, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner')\n",
    "    data.sort_values([session_key, time_key], inplace=True)\n",
    "\n",
    "    length = len(data['ItemId'])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dwell_time(df):\n",
    "   \n",
    "    times_t = np.roll(df['Time'], -1) # Take time row\n",
    "    times_dt  = df['Time']            # Copy, then displace by one\n",
    "    \n",
    "    diffs = np.subtract(times_t, times_dt) # Take the pairwise difference\n",
    "    \n",
    "    length = len(df['ItemId'])\n",
    "    \n",
    "    # cummulative offset start for each session\n",
    "    offset_sessions = np.zeros(df['SessionId'].nunique()+1, dtype=np.int32)\n",
    "    offset_sessions[1:] = df.groupby('SessionId').size().cumsum() \n",
    "    \n",
    "    offset_sessions = offset_sessions - 1\n",
    "    offset_sessions = np.roll(offset_sessions, -1)\n",
    "    \n",
    "    # session transition implies zero-dwell-time\n",
    "    # note: paper statistics do not consider null entries, \n",
    "    # though they are still checked when augmenting\n",
    "    np.put(diffs, offset_sessions, np.zeros((offset_sessions.shape)), mode='raise')\n",
    "        \n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paper statistics\n",
    "def get_statistics(dts):\n",
    "    filtered = np.array(list(filter(lambda x: int(x) != 0, dts)))\n",
    "    pd_dts = pd.DataFrame(filtered)\n",
    "    pd_dts.boxplot(vert=False, showfliers=False) # no outliers in boxplot\n",
    "    plt.show()\n",
    "    pd_dts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dwell_reps(df, dt, threshold=2000):\n",
    "    # Calculate d_ti/threshold + 1\n",
    "    # then add column to dataFrame\n",
    "    \n",
    "    dt //= threshold\n",
    "    dt += 1   \n",
    "    df['DwellReps'] = pd.Series(dt.astype(np.int64), index=dt.index)\n",
    "    #return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(df):    \n",
    "    col_names = list(df.columns.values)[:3]\n",
    "    print(col_names)\n",
    "    augmented = np.repeat(df.values, df['DwellReps'], axis=0) \n",
    "    print(augmented[0][:3])  \n",
    "    augmented = pd.DataFrame(data=augmented[:,:3],\n",
    "                             columns=col_names)\n",
    "    \n",
    "    dtype = {'SessionId': np.int64, \n",
    "             'ItemId': np.int64, \n",
    "             'Time': np.float32}\n",
    "    \n",
    "    for k, v in dtype.items():\n",
    "        augmented[k] = augmented[k].astype(v)\n",
    "                             \n",
    "    \n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SessionId', 'ItemId', 'Time']\n",
      "[        18      56788 1204200841]\n"
     ]
    }
   ],
   "source": [
    "# load RSC15 preprocessed train dataframe\n",
    "    PATH_TO_TRAIN = 'processed_train.csv'\n",
    "    train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "\n",
    "    new_df = preprocess_df(train_data)\n",
    "    dts = compute_dwell_time(new_df)\n",
    "\n",
    "    get_statistics(dts)\n",
    "\n",
    "    join_dwell_reps(new_df, dts, threshold=200000)\n",
    "\n",
    "    # Now, we augment the sessions copying each entry an additional (dwellReps[i]-1) times\n",
    "    df_aug = augment(new_df)\n",
    "    df_aug.to_csv(\"processed_augmented_train.csv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
