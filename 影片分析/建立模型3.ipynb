{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import psutil\n",
    "import sklearn\n",
    "import humanize\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import GPUtil as GPU\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers.core import Permute, Reshape, RepeatVector\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy\n",
    "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply, merge, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, data, sep='\\t', user_key='UserId', movie_key='MovieId', time_key='Time', n_samples=-1, movie_map=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: 讀取好的dataframe\n",
    "            sep: separator for the csv\n",
    "            user_key, movie_key, time_key: name of the fields corresponding to the users, movies, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            movie_map: mapping between movie IDs and movie indices\n",
    "            time_sort: whether to sort the users by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.user_key = user_key\n",
    "        self.movie_key = movie_key\n",
    "        self.time_sort = time_sort #False\n",
    "        self.add_item_indices(movie_map=movie_map)\n",
    "        self.df.sort_values([user_key, time_key], inplace=True)\n",
    "\n",
    "        #Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        #clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.user_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by user_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.user_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each users, sorted by session IDs\n",
    "            users_start_time = self.df.groupby(self.user_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(users_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.user_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    def add_item_indices(self, movie_map=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            movie_map (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if movie_map is None:\n",
    "            item_ids = self.df[self.movie_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            movie_map = pd.DataFrame({self.movie_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.movie_map = movie_map\n",
    "        self.df = pd.merge(self.df, self.movie_map, on=self.movie_key, how='inner')\n",
    "        \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.movie_map.MovieId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():   \n",
    "    emb_size = 50\n",
    "    hidden_units = 100\n",
    "    size = emb_size\n",
    "\n",
    "    inputs = Input(batch_shape=(batch_size, 1, num_movies))\n",
    "    gru, gru_states = CuDNNGRU(hidden_units, stateful=True, return_state=True)(inputs)# drop1) #\n",
    "    drop2 = Dropout(0.25)(gru)\n",
    "    predictions = Dense(num_movies, activation='softmax')(drop2)\n",
    "    \n",
    "    model = Model(input=inputs, output=[predictions])\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
    "    model.summary()\n",
    "\n",
    "    filepath='DwellTimeModel_checkpoint.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "    callbacks_list = []\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_states(model):\n",
    "    return [K.get_value(s) for s,_ in model.state_updates]\n",
    "\n",
    "\n",
    "def set_states(model, states):\n",
    "    for (d,_), s in zip(model.state_updates, states):\n",
    "        K.set_value(d, s)\n",
    "\n",
    "\n",
    "def get_recall(model, loader, epoch, train_generator_map, recall_k=20):\n",
    "\n",
    "    test_dataset = SessionDataset(test_data, movie_map=train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    suma_baseline = 0\n",
    "\n",
    "    for feat, label, mask in test_generator:\n",
    "\n",
    "        input_oh = to_categorical(feat, num_classes=loader.num_movies) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "        target_oh = to_categorical(label, num_classes=loader.num_movies)\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        if n%100 == 0:\n",
    "            try:\n",
    "                print(\"{}:{}\".format(n, suma/n))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx] \n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
    "            idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "            if idx2[0] in idx1:\n",
    "                suma += 1\n",
    "\n",
    "    print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))\n",
    "\n",
    "\n",
    "def get_mrr(model, loader,epoch,train_generator_map, mrr_k=20):\n",
    "\n",
    "    test_dataset = SessionDataset(test_data, movie_map = train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    suma_baseline = 0\n",
    "\n",
    "    for feat, label, mask in test_generator:\n",
    "        input_oh = to_categorical(feat, num_classes=loader.num_movies) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "        target_oh = to_categorical(label, num_classes=loader.num_movies)\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        if n%100 == 0:\n",
    "            try:\n",
    "                print(\"{}:{}\".format(n, suma/n))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx] \n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            idx1 = pred_row.argsort()[-mrr_k:][::-1]\n",
    "            idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "            if idx2[0] in idx1:\n",
    "                suma += 1/int((np.where(idx1 == idx2[0])[0]+1))        \n",
    "\n",
    "    print(\"MRR@{} epoch {}: {}\".format(mrr_k, epoch, suma/n))\n",
    "\n",
    "\n",
    "def train_model(model, save_weights = False, path_to_weights = True):\n",
    "    train_dataset = SessionDataset(train_data)\n",
    "    \n",
    "    print(\"train_dataset \\n\", train_dataset)\n",
    "\n",
    "    model_to_train = model\n",
    "\n",
    "    with tqdm(total=train_samples_qty) as pbar: #顯示處理進度條，每一個user處理一次\n",
    "        for epoch in range(1, 10):  #跑9次\n",
    "            \n",
    "            if path_to_weights: # \n",
    "                loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "                \n",
    "                \n",
    "            for feat, target, mask in loader:\n",
    "                #print(\"feat\", feat)\n",
    "                #print(\"target\", target)\n",
    "                #print(\"mask\", mask)\n",
    "\n",
    "                input_oh = to_categorical(feat, num_classes=loader.num_movies) \n",
    "                input_oh = np.expand_dims(input_oh, axis=1)\n",
    "                #print(\"input_oh\", input_oh)\n",
    "\n",
    "                target_oh = to_categorical(target, num_classes=loader.num_movies)\n",
    "                #print(\"target_oh\", target_oh)\n",
    "                \n",
    "                tr_loss = model_to_train.train_on_batch(input_oh, target_oh)\n",
    "                #print(\"tr_loss\", tr_loss)\n",
    "\n",
    "                real_mask = np.ones((batch_size, 1))\n",
    "                for elt in mask:\n",
    "                    real_mask[elt, :] = 0\n",
    "\n",
    "                hidden_states = get_states(model_to_train)[0]\n",
    "                hidden_states = np.multiply(real_mask, hidden_states)\n",
    "                hidden_states = np.array(hidden_states, dtype=np.float32)\n",
    "                #print(\"hidden_states\", hidden_states)\n",
    "                \n",
    "                model_to_train.layers[1].reset_states(hidden_states)\n",
    "\n",
    "                pbar.set_description(\"Epoch {0}. Loss: {1:.5f}\".format(epoch, tr_loss))\n",
    "                pbar.update(loader.done_users_counter)\n",
    "\n",
    "            # get metrics for epoch\n",
    "            get_recall(model_to_train, loader, epoch, train_dataset.movie_map)\n",
    "            get_mrr   (model_to_train, loader, epoch, train_dataset.movie_map)\n",
    "\n",
    "            # save model\n",
    "            if save_weights:\n",
    "                model_to_train.save('DwellTimeEpoch{}.h5'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataLoader:\n",
    "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"    \n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_users_counter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the users to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        user_key='UserId'\n",
    "        movie_key='MovieId'\n",
    "        time_key='TimeStamp'\n",
    "        self.num_movies = df[movie_key].nunique()+1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the users to be terminated\n",
    "        finished = False        \n",
    "        \n",
    "        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices (for embedding) for clicks where the first users start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = idx_input\n",
    "                target = idx_target\n",
    "                yield input, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many users should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_users_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]\n",
    "        \n",
    "        #print(\"num_movies \",self.num_movies)\n",
    "        #print(\"click_offsets \",click_offsets)\n",
    "        #print(\"session_idx_arr \",session_idx_arr)\n",
    "        #print(\"iters \",iters)\n",
    "        #print(\"maxiter \" ,maxiter)\n",
    "        #print(\"start \",start)\n",
    "        #print(\"end \",end)\n",
    "        #print(\"mask \",mask)\n",
    "        #print(\"finished \",finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training movies: 11619\n",
      "Unique dev movies: 10105\n",
      "Unique testing movies: 10366\n",
      "Training users: 19853\n",
      "Dev users: 5749\n",
      "Testing users: 5271\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (512, 1, 11619)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       [(512, 100), (512, 100)]  3516300   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (512, 100)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (512, 11619)              1173519   \n",
      "=================================================================\n",
      "Total params: 4,689,819\n",
      "Trainable params: 4,689,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "  0%|          | 0/19853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset \n",
      " <__main__.SessionDataset object at 0x7f3418ebe400>\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 5.00708:  98%|█████████▊| 19408/19853 [04:02<00:04, 89.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.100234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 5.00708:  98%|█████████▊| 19415/19853 [04:20<00:04, 89.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.0952734375\n",
      "38400:0.09270833333333334\n",
      "51200:0.09263671875\n",
      "64000:0.091875\n",
      "76800:0.0918359375\n",
      "89600:0.09114955357142857\n",
      "102400:0.091337890625\n",
      "115200:0.09184027777777778\n",
      "128000:0.092140625\n",
      "140800:0.09235085227272727\n",
      "153600:0.09272135416666667\n",
      "Recall@20 epoch 1: 0.09292829692556634\n",
      "12800:0.019512992638972212\n",
      "25600:0.018142292174334156\n",
      "38400:0.017779179198701957\n",
      "51200:0.017827835164857374\n",
      "64000:0.01754193255534555\n",
      "76800:0.017604920432064103\n",
      "89600:0.01763028926634335\n",
      "102400:0.017672822962482876\n",
      "115200:0.017744824815142814\n",
      "128000:0.01783160922375306\n",
      "140800:0.0178565254250353\n",
      "153600:0.01784887960399495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 7.34693:  98%|█████████▊| 19415/19853 [08:31<1:24:24, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 1: 0.01786657848998304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 4.12299: : 38823it [11:15, 117.65it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.118125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 4.12299: : 38830it [11:30, 117.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.1140625\n",
      "38400:0.11125\n",
      "51200:0.11130859375\n",
      "64000:0.111015625\n",
      "76800:0.11092447916666667\n",
      "89600:0.11065848214285715\n",
      "102400:0.11140625\n",
      "115200:0.11206597222222223\n",
      "128000:0.1123125\n",
      "140800:0.11232244318181818\n",
      "153600:0.11225911458333333\n",
      "Recall@20 epoch 2: 0.11237105582524272\n",
      "12800:0.023281662895766463\n",
      "25600:0.023217009787027402\n",
      "38400:0.022223348354803395\n",
      "51200:0.02208594757007383\n",
      "64000:0.021882454465374856\n",
      "76800:0.021960250547315166\n",
      "89600:0.02184225831950687\n",
      "102400:0.022099497669668632\n",
      "115200:0.022367177314190596\n",
      "128000:0.022406675552219393\n",
      "140800:0.022361517727785666\n",
      "153600:0.02223557297794026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Loss: 6.45690: : 38830it [15:40,  8.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 2: 0.02226387815024286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Loss: 3.87167: : 58238it [18:23, 122.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.12765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Loss: 3.87167: : 58245it [18:40, 122.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.1233203125\n",
      "38400:0.12059895833333334\n",
      "51200:0.12001953125\n",
      "64000:0.1186875\n",
      "76800:0.11828125\n",
      "89600:0.11797991071428572\n",
      "102400:0.11912109375\n",
      "115200:0.11971354166666667\n",
      "128000:0.1195234375\n",
      "140800:0.11950994318181818\n",
      "153600:0.11961588541666666\n",
      "Recall@20 epoch 3: 0.11955147653721683\n",
      "12800:0.026091440309896977\n",
      "25600:0.025343144730374384\n",
      "38400:0.0246395592671111\n",
      "51200:0.02471899779613925\n",
      "64000:0.024298768285646213\n",
      "76800:0.024318445486457452\n",
      "89600:0.024407743500734057\n",
      "102400:0.024802098706210116\n",
      "115200:0.025037391194029132\n",
      "128000:0.024942575030808885\n",
      "140800:0.025052647507187893\n",
      "153600:0.025039210650636554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Loss: 6.20623: : 58245it [22:48,  8.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 3: 0.025043308302798255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Loss: 3.65637: : 77660it [25:31, 116.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.130234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Loss: 3.65637: : 77660it [25:50, 116.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.126171875\n",
      "38400:0.12427083333333333\n",
      "51200:0.12326171875\n",
      "64000:0.1218125\n",
      "76800:0.12149739583333333\n",
      "89600:0.12125\n",
      "102400:0.123037109375\n",
      "115200:0.12363715277777777\n",
      "128000:0.1239375\n",
      "140800:0.12421875\n",
      "153600:0.12451822916666666\n",
      "Recall@20 epoch 4: 0.12463971480582524\n",
      "12800:0.02714745980820855\n",
      "25600:0.026846522146825744\n",
      "38400:0.02652192059625444\n",
      "51200:0.02623165200386024\n",
      "64000:0.025705010275832663\n",
      "76800:0.02566828192329864\n",
      "89600:0.025735277997473962\n",
      "102400:0.02610972640769789\n",
      "115200:0.026352241821791852\n",
      "128000:0.026284784790265584\n",
      "140800:0.02623655045261437\n",
      "153600:0.026216356997001006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Loss: 6.06589: : 77660it [29:57, 116.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 4: 0.02623726071903876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Loss: 3.59197: : 97068it [32:41, 116.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.130234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Loss: 3.59197: : 97075it [33:00, 116.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.1274609375\n",
      "38400:0.12565104166666666\n",
      "51200:0.12486328125\n",
      "64000:0.12371875\n",
      "76800:0.12388020833333334\n",
      "89600:0.123828125\n",
      "102400:0.125419921875\n",
      "115200:0.12585069444444444\n",
      "128000:0.1258125\n",
      "140800:0.12598011363636363\n",
      "153600:0.12651692708333334\n",
      "Recall@20 epoch 5: 0.12651066949838188\n",
      "12800:0.027197609088980824\n",
      "25600:0.026897133070995984\n",
      "38400:0.02626856206374714\n",
      "51200:0.026192876797594026\n",
      "64000:0.025670199480990735\n",
      "76800:0.025634520715432638\n",
      "89600:0.02581076114205678\n",
      "102400:0.02618882398176889\n",
      "115200:0.026367160673391943\n",
      "128000:0.02629716131177047\n",
      "140800:0.026303151829607397\n",
      "153600:0.026324741107546835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Loss: 5.96029: : 97075it [37:08,  8.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 5: 0.026354503477719067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Loss: 3.56784: : 116483it [39:51, 114.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.137109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Loss: 3.56784: : 116490it [40:11, 114.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.1330078125\n",
      "38400:0.130390625\n",
      "51200:0.12916015625\n",
      "64000:0.12775\n",
      "76800:0.12760416666666666\n",
      "89600:0.12762276785714285\n",
      "102400:0.1287890625\n",
      "115200:0.12935763888888888\n",
      "128000:0.1293828125\n",
      "140800:0.12926136363636365\n",
      "153600:0.12951171875\n",
      "Recall@20 epoch 6: 0.12959521642394822\n",
      "12800:0.02761591456949895\n",
      "25600:0.027548155889739547\n",
      "38400:0.02649091751952813\n",
      "51200:0.026284846556696384\n",
      "64000:0.025697627988048546\n",
      "76800:0.025822280161031573\n",
      "89600:0.026070362485536548\n",
      "102400:0.02646686744073304\n",
      "115200:0.02664915536111053\n",
      "128000:0.026579847818193933\n",
      "140800:0.026586664757651368\n",
      "153600:0.026620108010148004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Loss: 5.97729: : 116490it [44:16, 11.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 6: 0.026648131331948127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Loss: 3.47688: : 135905it [46:59, 116.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.134375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Loss: 3.47688: : 135905it [47:11, 116.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.13109375\n",
      "38400:0.12822916666666667\n",
      "51200:0.12767578125\n",
      "64000:0.125984375\n",
      "76800:0.1263671875\n",
      "89600:0.126640625\n",
      "102400:0.128017578125\n",
      "115200:0.12866319444444443\n",
      "128000:0.128609375\n",
      "140800:0.12855823863636365\n",
      "153600:0.12893880208333333\n",
      "Recall@20 epoch 7: 0.12902002427184467\n",
      "12800:0.02857523122430048\n",
      "25600:0.028141013670033865\n",
      "38400:0.02707428861571551\n",
      "51200:0.026982382308691308\n",
      "64000:0.026582802329933614\n",
      "76800:0.026533352611068536\n",
      "89600:0.02661944395433467\n",
      "102400:0.0269332654589624\n",
      "115200:0.02712180977597928\n",
      "128000:0.02705554164117572\n",
      "140800:0.027008375966167095\n",
      "153600:0.027026098243324435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Loss: 5.94701: : 135905it [51:23, 116.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 7: 0.027039523300886964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Loss: 3.41789: : 155313it [54:07, 116.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.137578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Loss: 3.41789: : 155320it [54:21, 116.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.131640625\n",
      "38400:0.12924479166666666\n",
      "51200:0.1289453125\n",
      "64000:0.127015625\n",
      "76800:0.12713541666666667\n",
      "89600:0.127421875\n",
      "102400:0.1292578125\n",
      "115200:0.12996527777777778\n",
      "128000:0.1299375\n",
      "140800:0.13011363636363638\n",
      "153600:0.13048177083333334\n",
      "Recall@20 epoch 8: 0.1306002224919094\n",
      "12800:0.028618198757753323\n",
      "25600:0.028266411724764278\n",
      "38400:0.027433265018265433\n",
      "51200:0.027373533393430963\n",
      "64000:0.02681065497419083\n",
      "76800:0.026654580763855216\n",
      "89600:0.02689540169717884\n",
      "102400:0.02736621517813131\n",
      "115200:0.027548340672139154\n",
      "128000:0.027438491610567252\n",
      "140800:0.027364839233804734\n",
      "153600:0.027422475895229573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Loss: 5.85908: : 155320it [58:33,  8.87s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 8: 0.027426659837399447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Loss: 3.31624: : 174728it [1:01:16, 115.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800:0.138203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Loss: 3.31624: : 174735it [1:01:31, 115.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600:0.13390625\n",
      "38400:0.13192708333333333\n",
      "51200:0.130625\n",
      "64000:0.129328125\n",
      "76800:0.12954427083333334\n",
      "89600:0.12982142857142856\n",
      "102400:0.13138671875\n",
      "115200:0.1321875\n",
      "128000:0.1322421875\n",
      "140800:0.1320028409090909\n",
      "153600:0.13239583333333332\n",
      "Recall@20 epoch 9: 0.1324585355987055\n",
      "12800:0.028832712789245023\n",
      "25600:0.028095617603749044\n",
      "38400:0.02729182868266846\n",
      "51200:0.027318726554686236\n",
      "64000:0.02680505349000359\n",
      "76800:0.026604746100969072\n",
      "89600:0.026660255416019237\n",
      "102400:0.027056620328798235\n",
      "115200:0.027245063146182247\n",
      "128000:0.02719429955951851\n",
      "140800:0.027215120384645134\n",
      "153600:0.027209141745917154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Loss: 3.31624: : 174735it [1:05:42, 44.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@20 epoch 9: 0.027240993072724835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    PATH_TO_TRAIN = 'processed_augmented_train.csv'\n",
    "    PATH_TO_DEV = 'processed_dev.csv'\n",
    "    PATH_TO_TEST = 'processed_test.csv'\n",
    "    train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'MovieId':np.int64})\n",
    "    dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'MovieId':np.int64})\n",
    "    test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'MovieId': np.int64})\n",
    "    \n",
    "    batch_size = 512\n",
    "    session_max_len = 100\n",
    "    embeddingp=False\n",
    "\n",
    "    num_movies = len(train_data['MovieId'].unique())+1\n",
    "    print(\"Unique training movies:\", num_movies)\n",
    "\n",
    "    dev_num_movies = len(dev_data['MovieId'].unique())+1\n",
    "    print(\"Unique dev movies:\", dev_num_movies)\n",
    "\n",
    "    test_num_movies = len(test_data['MovieId'].unique())+1\n",
    "    print(\"Unique testing movies:\", test_num_movies)\n",
    "\n",
    "    train_samples_qty = len(train_data['UserId'].unique()) \n",
    "    print(\"Training users:\", train_samples_qty)\n",
    "\n",
    "    dev_samples_qty = len(dev_data['UserId'].unique()) \n",
    "    print(\"Dev users:\",dev_samples_qty)\n",
    "\n",
    "    test_samples_qty = len(test_data['UserId'].unique())\n",
    "    print(\"Testing users:\", test_samples_qty)\n",
    "    \n",
    "    train_fraction = 1 # (1 / fraction) most recent session quantity to consider\n",
    "    dev_fraction = 1\n",
    "\n",
    "    train_offset_step=train_samples_qty//batch_size\n",
    "    dev_offset_step=dev_samples_qty//batch_size\n",
    "    test_offset_step=test_samples_qty//batch_size\n",
    "    aux = [0]\n",
    "    aux.extend(list(train_data['MovieId'].unique()))\n",
    "    itemids = np.array(aux)\n",
    "    itemidmap = pd.Series(data=np.arange(num_movies), index=itemids) \n",
    "    \n",
    "    model = create_model()\n",
    "    \n",
    "    train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
